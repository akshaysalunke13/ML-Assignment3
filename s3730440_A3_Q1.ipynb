{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH2319 Machine Learning\n",
    "## Semester 1, 2020\n",
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honour Code\n",
    "I solemnly swear that I have not discussed my assignment solutions with anyone in any way and the solutions I am submitting are my own personal work.\n",
    "\n",
    "Full Name: **Akshay Sunil Salunke** - *s3730440*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"A3_Q1_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Data preparation\n",
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check the shape of our df. Then we print first 5 rows. The target feature is `annual_income` which has 2 values, `low_income` & `high_income`. We consider `high_income` as positive target class i.e `1` hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the categorical features in new `df_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.drop(columns=['age', 'education_years', 'annual_income', 'row_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform `equal width binning` on continous features `age` & `education_years`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['low', 'mid', 'high']\n",
    "age_cat = pd.cut(df['age'], bins=3, labels=labels)\n",
    "ed_cat = pd.cut(df['education_years'], bins=3, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add these binned features to `df_new` dataframe. `age` and `education_years` are now categorical with following unique values: `low, mid, high`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['age_cat'], df_cat['education_years_cat'] = age_cat.astype(object), ed_cat.astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now append the target feature `annual_income` as the last column of `df_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_cat.join(df['annual_income'])\n",
    "df_all_cat = df_cat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so that we can see all the columns\n",
    "print(df_all_cat.shape)\n",
    "df_all_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run below in a separate cell!!!\n",
    "for col in df_all_cat.columns.tolist():  \n",
    "    print(col + ':')\n",
    "    print(df_all_cat[col].value_counts())\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "In this section, we perform **One Hot Encoding (OHE)** on our dataset.\n",
    "\n",
    "First, we check the datatypes for all our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cat.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform *OHE* on all columns except target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cat_ohe = pd.get_dummies(df_all_cat.drop(columns=['annual_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the target column after performing **integer encoding** on it. Instead of using `level_map` with `replace()` to integer encode, we use `get_dummies()` and drop the column `low_income`. Then we rename the column `high_income` generated by `get_dummies()` to `annual_income`, so we get similar results as if we had done integer encoding.\n",
    "\n",
    "You can get the same result by using `dropFirst=True` in `get_dummies()` and then reversing the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cat_ohe['annual_income'] = pd.get_dummies(df['annual_income']).drop(columns='low_income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_cat_ohe.shape)\n",
    "df_all_cat_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Bernoulli NB\n",
    "Here we fit a *Bernoulli NB* model with default parameters on our data, and score it again using same data. (Although this is cheating, this is what the assignment wants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df_all_cat_ohe.drop(columns=['annual_income']).values\n",
    "target = df_all_cat_ohe['annual_income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(Data, target)\n",
    "bs = bnb.score(Data, target)\n",
    "print(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[len(df)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the score for a *Bernoulli model* on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Gaussian NB\n",
    "Now we fit a *Guassian NB* model with default parameters on the dataset, and then calculate it's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(Data, target)\n",
    "gs = gnb.score(Data, target)\n",
    "print(gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Tuning the models\n",
    "### Task 1 - Tuning\n",
    "\n",
    "We write a function `test_params()` which accepts the `Data, target` and `clf`. `clf` is the classifier which runs with different values of parameters. \n",
    "\n",
    "**Parameters**: For *Bernoulli NB*, `alpha` is varied, whereas for *Gaussian NB*, `var_smoothing` is varied. \n",
    "\n",
    "This function returns `results` dataframe with all parameters `p` tested and their mean accuracy `test_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def test_params(Data, target, clf):\n",
    "    if isinstance(clf, BernoulliNB):\n",
    "        # param = alpha \n",
    "        params = [0, 0.5, 1, 2, 3, 5]\n",
    "    elif isinstance(clf, GaussianNB):\n",
    "        # param = var_smoothing\n",
    "        params = np.logspace(0,-9, num=10)\n",
    "    else:\n",
    "        raise Exception(\"Classifier not supported.\")\n",
    "\n",
    "    results = pd.DataFrame(params, columns=['p'])\n",
    "    results['test_score'] = None\n",
    "    for p in params:\n",
    "            if isinstance(clf, BernoulliNB):\n",
    "                clf.alpha = p\n",
    "            elif isinstance(clf, GaussianNB):\n",
    "                clf.var_smoothing = p\n",
    "\n",
    "            clf.fit(Data, target)\n",
    "            predict = clf.predict(Data)\n",
    "            score = metrics.accuracy_score(target, predict)\n",
    "            #print(\"Classifier:\", clf, \"Score:\", score)\n",
    "            results.loc[results['p']==p, 'test_score'] = score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call our `test_params()` function with `bnb`(*Bernoulli NB*) classifier and print the results df. \n",
    "\n",
    "Here, `p = alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_result = test_params(Data, target, bnb)\n",
    "bts = bnb_result['test_score'].max()\n",
    "bnb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, call our `test_params()` function with `gnb`(*Gaussian NB*) classifier and print the results df. \n",
    "\n",
    "Here, `p = var_smoothing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_result = test_params(Data, target, gnb)\n",
    "gts = gnb_result['test_score'].max()\n",
    "gnb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Plotting\n",
    "In this section we plot graphs for performance of both NB models with respect to different parameters.\n",
    "\n",
    "Below line plot shows performance of *Bernoulli NB* with different values for `alpa` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.Chart(bnb_result, \n",
    "          title='Bernoulli NB Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('p', title='alpha'),\n",
    "    alt.Y('test_score', title='Mean accuracy', scale=alt.Scale(zero=False))\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below line plot shows performance of *Gaussian NB* with different values for `var_smoothing` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(gnb_result, \n",
    "          title='Gaussian NB Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('p', title='var_smoothing'),\n",
    "    alt.Y('test_score', title='Mean accuracy', scale=alt.Scale(zero=False))\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E - Hybrid NB\n",
    "In this section we try to create an ensemble of *Bernoulli NB* and *Guassian NB*. This is because *Bernoulli NB* performs better on categorical features, whereas *Guasssian NB* performs better on continuos features that follow *Guassian Probability distribution*.\n",
    "\n",
    "We drop the ID like column `row_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"A3_Q1_train.csv\").drop(columns='row_id')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first seperate the categorical features in `Data` variable and then perform *One Hot Encoding* on them. Then we seperate the target feature in `target` variable and integer encode positive target class `high_income` to `1` and `low_income` to `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df2.select_dtypes(object).drop(columns='annual_income')\n",
    "Data = pd.get_dummies(Data)\n",
    "target = df2['annual_income'].replace({'high_income':1, 'low_income':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit a *Bernoulli NB* classifier on categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_clf = BernoulliNB()\n",
    "bnb_clf.fit(Data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we fit a *Guassian NB* classifier on continuos features `age` & `education_years`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data2 = df2.select_dtypes(int)\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "Data2 = PowerTransformer().fit_transform(Data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_clf = GaussianNB()\n",
    "gnb_clf.fit(Data2, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we predict the target on the same data using the abovee 2 classifiers, and store the target class probabilities in corresponding variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_prob = bnb_clf.predict_proba(Data)\n",
    "gnb_prob = gnb_clf.predict_proba(Data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we multiply both the probabilities given by *Bernoulli & Guassian NB*, in a variable `total_prob` by multiplying them. \n",
    "\n",
    "We are multiplying probabilities because of the **probability product rule** which states that *the probability of two (or more) independent events occurring together can be calculated by multiplying the individual probabilities of the events*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a 2d array, which contains probabilities where each element is in the format: \n",
    "# [prob. of low_income, prob. of high_income]\n",
    "total_prob = []\n",
    "for i in range(0, len(bnb_prob)):\n",
    "    total_prob.append(bnb_prob[i] * gnb_prob[i])\n",
    "\n",
    "# We add the 1 to prediction if probability of of high_income is >= 0.5, 0 otherwise\n",
    "prediction = []\n",
    "for p in total_prob:\n",
    "    if p[1] >= 0.5:\n",
    "        prediction.append(1)\n",
    "    else:\n",
    "        prediction.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate `accuracy_score` by comparing our predictions from *Hybrid NB* classifier vs actual target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs = metrics.accuracy_score(target, prediction)\n",
    "print(hs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 - Wrapping-up\n",
    "Below we summarize the results of various classifiers we have trained on our dataset in `df_summary` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary = pd.DataFrame(columns=['method', 'accuracy'])\n",
    "df_summary['method'] = ['Bernoulli NB (default)', 'Gaussian NB (default)', 'Bernoulli NB (tuned)', 'Gaussian NB (tuned)', 'Hybrid NB']\n",
    "df_summary['accuracy'] = [bs, gs, bts, gts, hs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i) Whether hyper-parameter tuning improves the performance of the Bernoulli and Gaussian NB models respectively. \n",
    "\n",
    "We tried tuning *Bernoulli NB* by varying the `alpha` parameter, but there was no performance increase.\n",
    "But *Guassian NB* shows an improvement in performance(~10%) after tuning it's `var_smoothing` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **(ii) Whether your Hybrid NB model has more predictive power than the (untuned) Bernoulli and Gaussian NB models respectively.**\n",
    "\n",
    "Based on `Fig. 1` & `Fig. 2`, we could say, out of the 2 continuos features `age` & `education_years`, `age` has curve similar to *bell curve* which signifies *normal distribution(Guassian probability distribution)*, whereas `education_years` has data biased towards right, which suggests data is not *normally distributed*.\n",
    "\n",
    "As we know *Guassian NB* performs well on *normal distribution* but only one out of two continuos features if *normally distributed*. Hence the *Bernoulli NB* performs well(0.83) as compared to *Hybrid NB*(0.77).\n",
    "\n",
    "But *Hybrid NB* performs better when compared with *Guassian NB*(0.72) since most (3 out of 5) features in our dataset are catgorical, and that's where *Bernoulli NB* shines, making *Hybrid NB*(0.77) a better performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "df['age'].plot(kind='hist', bins=5, title=\"Fig.1 - age frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['education_years'].plot(kind='hist', bins=10, title=\"Fig.2 - education_years frequency\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitvenvvenv05f04f3b75164ec790124b4fce9947c9",
   "display_name": "Python 3.7.7 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}