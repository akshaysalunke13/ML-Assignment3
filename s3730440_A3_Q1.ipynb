{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATH2319 Machine Learning\n",
    "## Semester 1, 2020\n",
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honour Code\n",
    "I solemnly swear that I have not discussed my assignment solutions with anyone in any way and the solutions I am submitting are my own personal work.\n",
    "\n",
    "Full Name: **Akshay Sunil Salunke** - *s3730440*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"A3_Q1_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A - Data preparation\n",
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first check the shape of our df. Then we print first 5 rows. The target feature is `annual_income` which has 2 values, `low_income` & `high_income`. We consider `high_income` as positive target class i.e `1` hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now extract all the categorical features in new `df_cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df.drop(columns=['age', 'education_years', 'annual_income', 'row_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform `equal width binning` on continous features `age` & `education_years`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['low', 'mid', 'high']\n",
    "age_cat = pd.cut(df['age'], bins=3, labels=labels)\n",
    "ed_cat = pd.cut(df['education_years'], bins=3, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add these binned features to `df_new` dataframe. `age` and `education_years` are now categorical with following unique values: `low, mid, high`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat['age_cat'], df_cat['education_years_cat'] = age_cat.astype(object), ed_cat.astype(object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now append the target feature `annual_income` as the last column of `df_cat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat = df_cat.join(df['annual_income'])\n",
    "df_all_cat = df_cat.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so that we can see all the columns\n",
    "print(df_all_cat.shape)\n",
    "df_all_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run below in a separate cell!!!\n",
    "for col in df_all_cat.columns.tolist():  \n",
    "    print(col + ':')\n",
    "    print(df_all_cat[col].value_counts())\n",
    "    print('********')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "In this section, we perform **One Hot Encoding (OHE)** on our dataset.\n",
    "\n",
    "First, we check the datatypes for all our columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cat.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform *OHE* on all columns except target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cat_ohe = pd.get_dummies(df_all_cat.drop(columns=['annual_income']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add the target column after performing **integer encoding** on it. Instead of using `level_map` with `replace()` to integer encode, we use `get_dummies()` and drop the column `low_income`. Then we rename the column `high_income` generated by `get_dummies()` to `annual_income`, so we get similar results as if we had done integer encoding.\n",
    "\n",
    "You can get the same result by using `dropFirst=True` in `get_dummies()` and then reversing the encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cat_ohe['annual_income'] = pd.get_dummies(df['annual_income']).drop(columns='low_income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all_cat_ohe.shape)\n",
    "df_all_cat_ohe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B - Bernoulli NB\n",
    "Here we fit a *Bernoulli NB* model with default parameters on our data, and score it again using same data. (Although this is cheating, this is what the assignment wants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df_all_cat_ohe.drop(columns=['annual_income']).values\n",
    "target = df_all_cat_ohe['annual_income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(Data, target)\n",
    "bnb.score(Data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the score for a *Bernoulli model* on our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C - Gaussian NB\n",
    "Now we fit a *Guassian NB* model with default parameters on the dataset, and then calculate it's score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(Data, target)\n",
    "gnb.score(Data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part D - Tuning the models\n",
    "### Task 1 - Tuning\n",
    "\n",
    "We write a function `best_params()` which accepts the `Data, target` and `clf`. `clf` is the classifier which runs with different values of parameters. \n",
    "\n",
    "**Parameters**: For *Bernoulli NB*, `alpha` is varied, whereas for *Gaussian NB*, `var_smoothing` is varied. \n",
    "\n",
    "This function returns `results` dataframe with all parameters `p` tested and their mean accuracy `test_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_params(Data, target, clf):\n",
    "    if isinstance(clf, BernoulliNB):\n",
    "        # param = alpha\n",
    "        params = [0, 0.5, 1, 2, 3, 5]\n",
    "    elif isinstance(clf, GaussianNB):\n",
    "        # param = var_smoothing\n",
    "        params = np.logspace(0,-9, num=10)\n",
    "    else:\n",
    "        raise Exception(\"Classifier not supported.\")\n",
    "\n",
    "    results = pd.DataFrame(params, columns=['p'])\n",
    "    results['test_score'] = None\n",
    "    for p in params:\n",
    "            if isinstance(clf, BernoulliNB):\n",
    "                clf.alpha = p\n",
    "            elif isinstance(clf, GaussianNB):\n",
    "                clf.var_smoothing = p\n",
    "\n",
    "            clf.fit(Data, target)\n",
    "            score = clf.score(Data, target)\n",
    "            #print(\"Classifier:\", clf, \"Score:\", score)\n",
    "            results.loc[results['p']==p, 'test_score'] = score\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then call our `best_params()` function with `bnb`(*Bernoulli NB*) classifier and print the results df. \n",
    "\n",
    "Here, `p = alpha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_result = best_params(Data, target, bnb)\n",
    "bnb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, call our `best_params()` function with `gnb`(*Gaussian NB*) classifier and print the results df. \n",
    "\n",
    "Here, `p = var_smoothing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_result = best_params(Data, target, gnb)\n",
    "gnb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - Plotting\n",
    "In this section we plot graphs for performance of both NB models with respect to different parameters.\n",
    "\n",
    "Below line plot shows performance of *Bernoulli NB* with different values for `alpa` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.Chart(bnb_result, \n",
    "          title='Bernoulli NB Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('p', title='alpha'),\n",
    "    alt.Y('test_score', title='Mean accuracy', scale=alt.Scale(zero=False))\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below line plot shows performance of *Gaussian NB* with different values for `var_smoothing` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(gnb_result, \n",
    "          title='Gaussian NB Performance Comparison'\n",
    "         ).mark_line(point=True).encode(\n",
    "    alt.X('p', title='var_smoothing'),\n",
    "    alt.Y('test_score', title='Mean accuracy', scale=alt.Scale(zero=False))\n",
    ").interactive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part E - Hybrid NB"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitvenvvenv05f04f3b75164ec790124b4fce9947c9",
   "display_name": "Python 3.7.7 64-bit ('venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}